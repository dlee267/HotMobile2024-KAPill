import numpy as np
import matplotlib.pyplot as plt
from scipy.io import loadmat
from pathlib import Path


data = dict()
# ppl = [f"MP{x}" for x in range(1,8)]
ppl = list()
ppl.append("Dong1")
locs = ["middle", "topright", "bottomright", "bottomleft", "topleft"]
# locs = ["middle"]
augs = ["M1", "NA", "R1"]
for pers in ppl:
    data[pers] = dict()
    for loc in locs:
        data[pers][loc] = dict()
        for aug in augs:
            # _arr = loadmat(f"{pers}_{aug}_{loc}_filteredEvents.mat")
            # print(_arr) filteredEvents
            # data[pers][loc][aug] = [x[0] for x in _arr["filteredEvents"].flatten()]
            data[pers][loc][aug] = np.load(f"{pers}_{aug}_{loc}.npy")
            


data["Dong1"]["middle"]["NA"].shape


for loc in locs:
    for aug in augs:
        ex = data["Dong1"][loc][aug][0]
        print(ex)
        plt.plot(ex)
        plt.title(f"{loc}-{aug}")
        plt.show()


# example MP2 middle R1 / NA
from scipy import signal
for loc in ["middle"]:
    ex = data["Dong1"][loc]["R1"][0]
    # raw
    plt.plot(np.arange(0,len(ex) / 2400,1/2400),ex)
    plt.savefig("raw_ex")
    plt.show()
    # stft
    # ex = data["Dong1"][loc]["R1"][0]
    f, t, Zxx = signal.stft(ex,2400, nperseg=20)
    plt.pcolormesh(t, f, np.abs(Zxx), vmin=0, vmax=ex.max(), shading='gouraud')
    plt.title('STFT Magnitude')
    plt.ylabel('Frequency [Hz]')
    plt.xlabel('Time [sec]')
    plt.savefig("stft_ex")
    plt.show()

    
    # band pass 400 - 600 Hz
    band = np.where(np.logical_and(f > 400, f < 600))
    f_band_pass = f[band]
    Zxx_band_pass = Zxx[band]
    # set to normalized scale
    exp = np.max(np.abs(Zxx_band_pass))
    # print(exp)
    Zxx_band_pass = Zxx_band_pass / exp
    print(f_band_pass.shape, t.shape, Zxx_band_pass.shape)
    plt.plot(t, np.abs(Zxx_band_pass[0]))
    plt.title('STFT w Band Pass')
    plt.ylabel('Amplitude')
    plt.xlabel('Time [sec]')
    plt.savefig("band_pass_ex")
    plt.show()
    


np.savetxt("sample_signal.csv", data["Dong1"][loc]["R1"][0], delimiter=',')


for loc in locs:
    ex2 = data["MP2"][loc]["NA"][0,0][0]
    f, t, Zxx = signal.stft(ex2,2400, nperseg=20)
    plt.pcolormesh(t, f, np.abs(Zxx), vmin=0, vmax=ex2.max(), shading='gouraud')
    plt.title('STFT Magnitude')
    plt.ylabel('Frequency [Hz]')
    plt.xlabel('Time [sec]')
    plt.show()
    plt.plot(np.arange(0,len(ex2) / 2400,1/2400),ex2)
    plt.show()
    # band pass 400 - 600 Hz
    band = np.where(np.logical_and(f > 400, f < 600))
    f_band_pass = f[band]
    Zxx_band_pass = Zxx[band]
    print(f_band_pass.shape, t.shape, Zxx_band_pass.shape)
    plt.plot(t, np.abs(Zxx_band_pass[0]))
    plt.title('STFT w Band Pass')
    plt.ylabel('Frequency [Hz]')
    plt.xlabel('Time [sec]')
    plt.show()


# example MP2 middle R1 / NA
from scipy import signal
for aug in augs:
    print(aug)
    # stft
    ex = data["Dong1"]["middle"][aug][0]
    ex = ex - ex.mean()
    
    # normalize raw signal
    eng = np.sqrt(np.sum(ex**2))
    # ex = ex / eng 
    
    f, t, Zxx = signal.stft(ex,2400, nperseg=25)
    plt.pcolormesh(t, f, np.abs(Zxx), vmin=0, vmax=ex.max(), shading='gouraud')
    plt.title('STFT Magnitude')
    plt.ylabel('Frequency [Hz]')
    plt.xlabel('Time [sec]')
    plt.show()
    # band pass 400 - 500 Hz
    band = np.where(np.logical_and(f > 400, f < 500))
    f_band_pass = f[band]
    Zxx_band_pass = Zxx[band]
    # moving average for filtering
    # print(Zxx_band_pass.shape)
    def moving_average(a, n=3):
        ret = np.cumsum(a, dtype=float)
        ret[n:] = ret[n:] - ret[:-n]
        return ret[n - 1:] / n
    Zxx_band_pass = moving_average(Zxx_band_pass[0], n=3)

    # get abs
    Zxx_band_pass = np.abs(Zxx_band_pass)
    
    # set to normalized scale
    exp = np.max(Zxx_band_pass)
    # print(exp)
    Zxx_band_pass = Zxx_band_pass / exp
    n_peaks = list()
    for i in np.arange(0.1,1,0.1):
        peaks = signal.find_peaks(Zxx_band_pass, height=i, distance=7)
        n_peaks.append(len(peaks))
        # print(peaks[0], Zxx_band_pass)
        peak_values = Zxx_band_pass[peaks[0]]
        if len(peak_values) >= 2:
            print(peak_values[0] / peak_values[1])
    # finding peaks
    # scipy.signal.find_peaks(Zxx_band_pass, height=)
    
    # print(f_band_pass.shape, t.shape, Zxx_band_pass.shape)
    # plt.plot(t[:len(Zxx_band_pass)], Zxx_band_pass)
    plt.plot(Zxx_band_pass)
    plt.title('STFT w Band Pass')
    plt.ylabel('Amplitude')
    plt.xlabel('Time [sec]')
    plt.show()
    # raw
    plt.plot(np.arange(0,len(ex) / 2400,1/2400),ex)
    plt.show()


# stft
ex = data["MP2"]["middle"]["R1"][0,0][0]
f, t, Zxx = signal.stft(ex,2400, nperseg=20)
plt.pcolormesh(t, f, np.abs(Zxx), vmin=0, vmax=ex.max(), shading='gouraud')
plt.title('STFT Magnitude')
plt.ylabel('Frequency [Hz]')
plt.xlabel('Time [sec]')
plt.show()

ex = data["MP2"]["middle"]["M1"][0,0][0] # 560
f, t, Zxx = signal.stft(ex,2400, nperseg=20)
plt.pcolormesh(t, f, np.abs(Zxx), vmin=0, vmax=ex.max(), shading='gouraud')
plt.title('STFT Magnitude')
plt.ylabel('Frequency [Hz]')
plt.xlabel('Time [sec]')
plt.show()

ex = data["MP2"]["middle"]["NA"][0,0][0]
f, t, Zxx = signal.stft(ex,2400, nperseg=20)
plt.pcolormesh(t, f, np.abs(Zxx), vmin=0, vmax=ex.max(), shading='gouraud')
plt.title('STFT Magnitude')
plt.ylabel('Frequency [Hz]')
plt.xlabel('Time [sec]')
plt.show()



# example MP2 middle R1 / NA
from scipy import signal
for loc in locs:
    # stft
    ex = data["MP2"][loc]["M1"][0,0][0]
    f, t, Zxx = signal.stft(ex,2400, nperseg=25)
    plt.pcolormesh(t, f, np.abs(Zxx), vmin=0, vmax=ex.max(), shading='gouraud')
    plt.title('STFT Magnitude')
    plt.ylabel('Frequency [Hz]')
    plt.xlabel('Time [sec]')
    plt.show()
    
    # band pass 400 - 500 Hz
    band = np.where(np.logical_and(f > 400, f < 500))
    f_band_pass = f[band]
    Zxx_band_pass = Zxx[band]
    print(f_band_pass.shape, t.shape, Zxx_band_pass.shape)
    plt.plot(t, np.abs(Zxx_band_pass[0]))
    plt.title('STFT w Band Pass')
    plt.ylabel('Amplitude')
    plt.xlabel('Time [sec]')
    plt.show()
    # raw
    plt.plot(np.arange(0,len(ex) / 2400,1/2400),ex)
    plt.show()


# example MP2 middle R1 / NA
from scipy import signal, stats, optimize
for pers in ppl:
    for loc in locs:
        for aug in ["NA", "R1", "M1"]:
            for i in range(10):
                ex = data[pers][loc][aug][i]
                plt.pcolormesh(t, f, np.abs(Zxx), vmin=0, vmax=ex.max(), shading='gouraud')
                plt.title(f'STFT Magnitude {loc} {aug} {i}')
                plt.ylabel('Frequency [Hz]')
                plt.xlabel('Time [sec]')
                plt.show()
            n_features = list()
            for i in range(len(data[pers][loc][aug])):
                # stft
                ex = data[pers][loc][aug][i]
                ex = ex - ex.mean()
        
                # normalize raw signal
                eng = np.sqrt(np.sum(ex**2))
                # ex = ex / eng 
        
                f, t, Zxx = signal.stft(ex,2400, nperseg=25)
                # plt.pcolormesh(t, f, np.abs(Zxx), vmin=0, vmax=ex.max(), shading='gouraud')
                # plt.title(f'STFT Magnitude {loc} {aug}')
                # plt.ylabel('Frequency [Hz]')
                # plt.xlabel('Time [sec]')
                # plt.show()
        
                # band pass 400 - 500 Hz
                # band = np.where(np.logical_and(f > 400, f < 500))
                band = np.where(np.logical_and(f >= 200, f <= 600))
                f_band_pass = f[band]
                Zxx_band_pass = Zxx[band]
                # moving average for filtering
                # print(Zxx_band_pass.shape)
                def moving_average(a, n=3):
                    ret = np.cumsum(a, dtype=float)
                    ret[n:] = ret[n:] - ret[:-n]
                    return ret[n - 1:] / n
                Zxx_band_pass = moving_average(Zxx_band_pass[0], n=3)
    
                # get abs
                Zxx_band_pass = np.abs(Zxx_band_pass)
        
                # set to normalized scale
                exp = np.max(Zxx_band_pass)
                Zxx_band_pass = Zxx_band_pass / exp

                # get number of peaks
                n_peaks = list()
                for i in np.arange(0.1,0.4,0.1):
                    peaks = signal.find_peaks(Zxx_band_pass, height=i, distance=7)[0]
                    n_peaks.append(len(peaks))
                    # print(peaks[0], Zxx_band_pass)

                # get line fit
                all_x_peaks = signal.find_peaks(Zxx_band_pass, height=0.1, distance=7)[0]
                all_y_peaks = Zxx_band_pass[all_x_peaks]
                res = stats.linregress(all_x_peaks, all_y_peaks)
                if not np.isnan(res.slope):
                    n_peaks.append(res.slope * 100)
                else:
                    n_peaks.append(-20)
                # print(res.intercept, res.slope)

                # exponential fit
                def func(x, a, b, c):
                    return a * np.exp(-b * x) + c
                # if len(all_x_peaks) >= 3:
                try:
                    popt, pcov = optimize.curve_fit(func, all_x_peaks, all_y_peaks)
                    n_peaks.append(popt[1])
                except:
                    # print("err")
                    n_peaks.append(1)

                # OPTIONAL CHECK
                # print(all_x_peaks, all_y_peaks)
                # plt.plot(Zxx_band_pass)
                # xpos = np.arange(len(Zxx_band_pass))
                # plt.plot(xpos, res.intercept + res.slope*xpos, 'r')
                # plt.plot(xpos, func(xpos, *popt), 'r-', label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))
                # plt.title('STFT w Band Pass')
                # plt.ylabel('Amplitude')
                # plt.xlabel('Time [sec]')
                # plt.legend()
                # plt.show()
                
                # get 1st peak ratio
                if len(all_y_peaks) >= 2:
                    n_peaks.append(all_y_peaks[0] / all_y_peaks[1])
                else:
                    n_peaks.append(100)
                # print(n_peaks)
                n_features.append(n_peaks)
            n_features = np.array(n_features)
            # print(n_features)
            print(f"{pers}_{aug}_{loc}_features.npy")
            np.save(f"{pers}_{aug}_{loc}_features.npy",n_features)


from sklearn import svm
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from tuning import svmTuning
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, MinMaxScaler


# GLOBAL VARS
prefix = "Mix_2_ALL_Features"
locations = ["middle", "topright", "bottomright", "bottomleft", "topleft"]
# locations = ['middle']
locs = locations
# augments = ["NA", "R1", "R2", "M1", "M2"]
augments = ["NA", "R1", "M1"]
augment_keys = {k:n for n,k in enumerate(augments)}
people = ppl.copy()
# people.remove("MP5")

# WHOLE DATASET
location_variance = {}

for person in people:
    location_variance[person] = dict()
    d = location_variance[person]
    for location in locations:
        d[location] = {}
        for augment in augments:
            _arr = np.load(f"{person}_{augment}_{location}_features.npy")
            if _arr.shape[0] == 0:
                print(f"{person}_{augment}_{location}_features.npy")
            scaler = StandardScaler().set_output(transform="pandas")
            # scaler = MinMaxScaler().set_output(transform="pandas")
            _arr = np.c_[scaler.fit_transform(_arr[:,:7]), _arr[:,7:]]
            # print(_arr)
            # assert _arr.shape == (10,2), f"{prefix}_{person}_{augment}_{location}.csv in wrong shape: {_arr.shape}"
            # print(len(_arr))
            _arr = np.c_[_arr, np.ones(_arr.shape[0])*augment_keys[augment]]

            assert not np.isnan(_arr[:,7:]).any(), f"contains nan in {prefix}_{person}_{augment}_{location}.csv"

            
            # print(_arr)
            d[location][augment] = _arr

# EXAMPLE: M1_ALL_AUGMENTS_ALL_LOCATIONS
def tune(X_train, y_train, X_test, y_test):
    '''tunes both with linear and rbf. For our purposese, only show rsvm_acc'''
    # maxes = np.argsort(dataset[:,102:-1])[-10:]
    # define classifers
    clf_lsvm = SVC(kernel = 'linear')
    clf_rsvm = SVC(kernel = 'rbf')
    
    # tune and print result (tuned_lsvm, tuned_rsvm, lsvm_acc, rsvm_acc)
    results = svmTuning(X_train, y_train, X_test, y_test, clf_lsvm, clf_rsvm, 3)
    # print(results[2:])
    return results

def get_dataset(d):
    '''return copy of the dataset'''
    dataset = []
    for loc in locations:
        for aug in augments:
            dataset.extend(d[loc][aug])
    return np.array(dataset)

def get_dataset_location_picky(d, l):
    '''returns leave one out based on location of the dataset'''
    train = []
    test = []
    for loc in locations:
        if l == loc: 
            for aug in augments:
                test.extend(d[loc][aug])
            continue
        else:
            for aug in augments:
                train.extend(d[loc][aug])
    return np.array(train), np.array(test)



get_dataset(location_variance["Dong1"])


# KAP_LOO
accuracy_results = list()
confusions = list()
for percentage in np.arange(0.1, 0.9, 0.2):
    percentage = np.round(percentage, 1)
    print(percentage)
    for loc in locs:
        _r = list()
        for _i in range(1):
            for person in people:
                # print(percentage, _i, bottle)
                rest_dataset, loc_dataset = get_dataset_location_picky(location_variance[person], loc)
                # total_size = len(rest_dataset) + len(loc_dataset)
                # X_train, _, y_train, _ = train_test_split(rest_dataset[:, :-1], rest_dataset[:, -1],
                #                                           train_size=percentage * total_size / len(rest_dataset))
                # _, X_test, _, y_test = train_test_split(loc_dataset[:, :-1], loc_dataset[:, -1],
                #                                         test_size=0.1 * total_size / len(loc_dataset))
                _, clf, _, acc = tune(rest_dataset[:, :-1], rest_dataset[:, -1], loc_dataset[:, :-1], loc_dataset[:, -1])
                _r.append(acc)
                confusions.append(confusion_matrix(y_test, clf.predict(X_test)))
        accuracy_results_percentage = [percentage, np.average(_r), np.std(_r)]
        print(accuracy_results_percentage)
        accuracy_results.append(accuracy_results_percentage)
np.save("confusions_bottles.npy", confusions)
np.save("results.npy", accuracy_results)



accuracy_results = list()
confusions = list()
for percentage in np.arange(0.1, 0.9, 0.2):
    percentage = np.round(percentage, 1)
    print(percentage)
    for loc in locs:
        _r = list()
        for _i in range(20):
            for person in people:
                # print(percentage, _i, bottle)
                loc_dataset = get_dataset(location_variance[person])
                total_size = len(rest_dataset) + len(loc_dataset)
                X_train, X_test, y_train, y_test = train_test_split(loc_dataset[:, :-1], loc_dataset[:, -1],
                                                          train_size=percentage, test_size=1)
                _, clf, _, acc = tune(X_train, y_train, X_test, y_test)
                _r.append(acc)
                confusions.append(confusion_matrix(y_test, clf.predict(X_test)))
        accuracy_results_percentage = [percentage, np.average(_r), np.std(_r)]
        print(accuracy_results_percentage)
        accuracy_results.append(accuracy_results_percentage)


percentage = np.round(0.9, 1)
print(percentage)
for loc in locs:
    _r = list()
    for _i in range(20):
        for person in people:
            # print(percentage, _i, bottle)
            loc_dataset = get_dataset(location_variance[person])
            total_size = len(rest_dataset) + len(loc_dataset)
            X_train, X_test, y_train, y_test = train_test_split(loc_dataset[:, :-1], loc_dataset[:, -1],
                                                      train_size=percentage, test_size=1)
            _, clf, _, acc = tune(X_train, y_train, X_test, y_test)
            _r.append(acc)
            confusions.append(confusion_matrix(y_test, clf.predict(X_test)))
    accuracy_results_percentage = [percentage, np.average(_r), np.std(_r)]
    print(accuracy_results_percentage)
    accuracy_results.append(accuracy_results_percentage)


percentage = np.round(0.9, 1)
print(percentage)
for loc in locs:
    _r = list()
    for _i in range(20):
        for person in people:
            # print(percentage, _i, bottle)
            loc_dataset = get_dataset(location_variance[person])
            total_size = len(loc_dataset)
            X_train, X_test, y_train, y_test = train_test_split(loc_dataset[:, :-1], loc_dataset[:, -1],
                                                      train_size=percentage, test_size=1)
            _, clf, _, acc = tune(X_train, y_train, X_test, y_test)
            _r.append(acc)
            # confusions.append(confusion_matrix(y_test, clf.predict(X_test)))
    accuracy_results_percentage = [percentage, np.average(_r), np.std(_r)]
    print(accuracy_results_percentage)
    # accuracy_results.append(accuracy_results_percentage)


import pickle
percentage = np.round(1.0, 1)
print(percentage)
X = list()
y = list()
for loc in locs:
    _r = list()
    for person in people:
        # print(percentage, _i, bottle)
        loc_dataset = get_dataset(location_variance[person])
        # total_size = len(rest_dataset) + len(loc_dataset)
        X_train, y_train = loc_dataset[:, :-1], loc_dataset[:, -1]
        X.extend(X_train)
        y.extend(y_train)
_, clf, _, acc = tune(X, y, X, y)
_r.append(acc)
# print()
with open("model.pickle", "wb") as f:
    print(clf)
    pickle.dump(clf, f)
# confusions.append(confusion_matrix(y_test, clf.predict(X_test)))
accuracy_results_percentage = [percentage, np.average(_r), np.std(_r)]
print(accuracy_results_percentage)
# accuracy_results.append(accuracy_results_percentage)


with open("model.pickle", "rb") as f:
    clf = pickle.load(f)
clf.predict(X)


# GLOBAL VARS
prefix = "Mix_2_ALL_Features"
locations = ["middle", "topright", "bottomright", "bottomleft", "topleft"]
locs = locations
# augments = ["NA", "R1", "R2", "M1", "M2"]
augments = ["NA", "R1", "M1"]
augment_keys = {k:n for n,k in enumerate(augments)}
people = ppl.copy()
people.remove("MP5")
# KAP no LOO (90% training)
# MP1 MP2...
for person in ["MP1", "Dong1"]:
    print(person)
    for loc in locs:
        _r = list()
        for _i in range(20):
            # print(percentage, _i, bottle)
            # loc_dataset = get_dataset(location_variance[person])

            rest_dataset, loc_dataset = get_dataset_location_picky(location_variance[person], loc)
            locX_train, locX_test, locy_train, locy_test = train_test_split(loc_dataset[:, :-1], loc_dataset[:, -1],
                                                      train_size=0.9, random_state=_i, stratify=loc_dataset[:, -1])
            restX_train, restX_test, resty_train, resty_test = train_test_split(rest_dataset[:, :-1], rest_dataset[:, -1],
                                                      train_size=0.9, random_state=_i, stratify=rest_dataset[:, -1])
            # print(locy_test)
            # _, clf, _, acc = tune(np.concatenate((restX_train,locX_train)), np.concatenate((resty_train,locy_train)), locX_test, locy_test)
            _, clf, _, acc = tune(locX_train, locy_train, locX_test, locy_test)
            _r.append(acc)
            # confusions.append(confusion_matrix(y_test, clf.predict(X_test)))
        accuracy_results_percentage = [loc, np.average(_r), np.std(_r)]
        print(accuracy_results_percentage)
    # accuracy_results.append(accuracy_results_percentage)


# KAP no LOO (90% training)
# MP1 MP2...
for person in ["THREE"]:
    print(person)
    for loc in locs:
        _r = list()
        for _i in range(20):
            # print(percentage, _i, bottle)
            # loc_dataset = get_dataset(location_variance[person])

            rest_dataset, loc_dataset = get_dataset_location_picky(location_variance[person], loc)
            locX_train, locX_test, locy_train, locy_test = train_test_split(loc_dataset[:, :-1], loc_dataset[:, -1],
                                                      train_size=0.9, random_state=_i, stratify=loc_dataset[:, -1])
            # restX_train, restX_test, resty_train, resty_test = train_test_split(rest_dataset[:, :-1], rest_dataset[:, -1],
                                                      # train_size=0.9, random_state=_i, stratify=rest_dataset[:, -1])
            # print(locy_test)
            # _, clf, _, acc = tune(np.concatenate((restX_train,locX_train)), np.concatenate((resty_train,locy_train)), locX_test, locy_test)
            _, clf, _, acc = tune(locX_train, locy_train, locX_test, locy_test)
            _r.append(acc)
            # confusions.append(confusion_matrix(y_test, clf.predict(X_test)))
        accuracy_results_percentage = [loc, np.average(_r), np.std(_r)]
        print(accuracy_results_percentage)
    # accuracy_results.append(accuracy_results_percentage)



# KAP no LOO (90% training) w/ f=(200,600)
# MP1 MP2...
for person in ["Dong1"]:
    print(person)
    for loc in locs:
        _r = list()
        for _i in range(20):
            # print(percentage, _i, bottle)
            # loc_dataset = get_dataset(location_variance[person])

            rest_dataset, loc_dataset = get_dataset_location_picky(location_variance[person], loc)
            locX_train, locX_test, locy_train, locy_test = train_test_split(loc_dataset[:, :-1], loc_dataset[:, -1],
                                                      train_size=0.9, random_state=_i, stratify=loc_dataset[:, -1])
            restX_train, restX_test, resty_train, resty_test = train_test_split(rest_dataset[:, :-1], rest_dataset[:, -1],
                                                      train_size=0.9, random_state=_i, stratify=rest_dataset[:, -1])
            # print(locy_test)
            # _, clf, _, acc = tune(np.concatenate((restX_train,locX_train)), np.concatenate((resty_train,locy_train)), locX_test, locy_test)
            _, clf, _, acc = tune(locX_train, locy_train, locX_test, locy_test)
            _r.append(acc)
            # confusions.append(confusion_matrix(y_test, clf.predict(X_test)))
        accuracy_results_percentage = [loc, np.average(_r), np.std(_r)]
        print(accuracy_results_percentage)
    # accuracy_results.append(accuracy_results_percentage)


Dong1
['middle', 0.7, 0.0881917103688197]
['topright', 0.7699999999999998, 0.09769567259835231]
['bottomright', 0.8133333333333335, 0.09333333333333334]
['bottomleft', 0.9, 0.06146362971528591]
['topleft', 0.6933333333333335, 0.10413666234542207]
[0.1, 0.74, 0.0]
[0.1, 0.82, 0.0]
[0.1, 0.8523489932885906, 0.0]
[0.1, 0.38666666666666666, 0.0]
[0.1, 0.7466666666666667, 0.0]


# KAP_LOO
accuracy_results = list()
confusions = list()
# percentage = 0.9
# print(percentage)
for person in people:
    print(person)
    for loc in locs:
        _r = list()
        for _i in range(20):
                # print(percentage, _i, bottle)
                rest_dataset, loc_dataset = get_dataset_location_picky(location_variance[person], loc)
                # total_size = len(rest_dataset) + len(loc_dataset)
                # X_train, _, y_train, _ = train_test_split(rest_dataset[:, :-1], rest_dataset[:, -1],
                #                                           train_size=percentage * total_size / len(rest_dataset))
                # _, X_test, _, y_test = train_test_split(loc_dataset[:, :-1], loc_dataset[:, -1],
                #                                         test_size=0.1 * total_size / len(loc_dataset))
                _, clf, _, acc = tune(rest_dataset[:, :-1], rest_dataset[:, -1], loc_dataset[:, :-1], loc_dataset[:, -1])
                _r.append(acc)
                # confusions.append(confusion_matrix(y_test, clf.predict(X_test)))
        accuracy_results_percentage = [loc, np.average(_r), np.std(_r)]
        print(accuracy_results_percentage)
        accuracy_results.append(accuracy_results_percentage)
# np.save("confusions_bottles.npy", confusions)
# np.save("results.npy", accuracy_results)



