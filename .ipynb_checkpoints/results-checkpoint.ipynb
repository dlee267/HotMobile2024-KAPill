{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c77f74e7-ad10-477f-8151-d494fbf75321",
   "metadata": {},
   "source": [
    "# Preprocessing and Feature Extraction\n",
    "\n",
    "[Reading the dataset folders](#Reading-the-dataset-folders):\n",
    "- KAP, the dataset that contains events from 3 kinetic-augmented bases \"NA\", \"R1\", \"M1\"\n",
    "- NO_KAA, the dataset that contains events from 3 different pill bottles without augmentation \"B1\", \"B2\", \"B3\"\n",
    "\n",
    "[Preprocessing each event](#Preprocessing-each-event):\n",
    "- We normalize based on the greatest peak of each event detected\n",
    "- We isolate a frequency band between 500 Hz - 600 Hz\n",
    "\n",
    "[Feature Extraction](#Feature-Extraction):\n",
    "- energy decay, in the form of clamped peak count at different amplitude levels\n",
    "- peak interval comparing the first 3 peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda7e193-6af0-4c76-b1fe-9bc644475285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from pathlib import Path\n",
    "\n",
    "# map for folder and file locations\n",
    "input_files = {\"KAA/events\": (((\"KAA\",\"npy\"),),(\"NA\", \"R1\", \"M1\"),(\"middle\", \"topright\", \"bottomright\", \"bottomleft\", \"topleft\")),\n",
    "              \"NO_KAA/events\": (((\"no_KAA\",\"mat\"),),(\"B1\", \"B2\", \"B3\"),(\"middle\", \"topright\", \"bottomright\", \"bottomleft\", \"topleft\")),\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816168f7-1f37-4731-8fa5-8f3d5169bc40",
   "metadata": {},
   "source": [
    "## Reading the dataset folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ba0569f-e45e-4e23-843d-7b71a67b1f03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7508/399772107.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data[pers_name][loc][aug] = np.array([x[0] for x in _arr[\"filteredEvents\"].flatten()])\n"
     ]
    }
   ],
   "source": [
    "# read all data\n",
    "data = dict()\n",
    "for folder,info in input_files.items():\n",
    "    ppl, augs, locs = info\n",
    "    # print(ppl)\n",
    "    for pers in ppl:\n",
    "        pers_name, filetype = pers\n",
    "        data[pers_name] = dict()\n",
    "        for loc in locs:\n",
    "            data[pers_name][loc] = dict()\n",
    "            for aug in augs:\n",
    "                if filetype == \"npy\":\n",
    "                    data[pers_name][loc][aug] = np.load(f\"./{folder}/{pers_name}_{aug}_{loc}.npy\")\n",
    "                elif filetype == \"mat\":\n",
    "                    _arr = loadmat(f\"./{folder}/{pers_name}_{aug}_{loc}.mat\")\n",
    "                    data[pers_name][loc][aug] = np.array([x[0] for x in _arr[\"filteredEvents\"].flatten()])\n",
    "                elif filetype == \"csv\":\n",
    "                    data[pers_name][loc][aug] = np.loadtxt(f\"./{folder}/{pers_name}_{aug}_{loc}.csv\", delimiter=',')\n",
    "                # print(data[pers_name][loc][aug])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b6bf32-accf-4e90-a9e7-97cd061424b2",
   "metadata": {},
   "source": [
    "## Preprocessing each event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd2ce683-4e53-4725-bcfe-fbb6cff16e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal, stats, optimize\n",
    "from pathlib import Path\n",
    "preprocessed_data = dict()\n",
    "for folder,info in input_files.items():\n",
    "    ppl, augs, locs = info\n",
    "    for pers in ppl:\n",
    "        pers_name, filetype = pers\n",
    "        preprocessed_data[pers_name] = dict()\n",
    "        for loc in locs:\n",
    "            preprocessed_data[pers_name][loc] = dict()\n",
    "            for aug in augs:\n",
    "                _output = list()\n",
    "                _ffts = list()\n",
    "                for i in range(len(data[pers_name][loc][aug])):\n",
    "                    \n",
    "                    # stft\n",
    "                    ex = data[pers_name][loc][aug][i]\n",
    "                    ex = ex - ex.mean()\n",
    "                    \n",
    "                    # normalize raw signal\n",
    "                    eng = np.sqrt(np.sum(ex**2))\n",
    "                    ex = ex / eng\n",
    "                    tmp_fft = np.fft.fft(ex, n = 2350)\n",
    "                    abs_fft = np.abs(tmp_fft)\n",
    "                    _ffts.append(abs_fft[:1174])\n",
    "                    \n",
    "                    # stft\n",
    "                    f, t, Zxx = signal.stft(ex,2400, nperseg=25)\n",
    "                    Zxx = np.abs(Zxx)\n",
    "                    \n",
    "                    # band pass 500 Hz - 600 Hz\n",
    "                    band = np.where(np.logical_and(f > 700, f < 800))\n",
    "                    f_band_pass = f[band]\n",
    "                    Zxx_band_pass = Zxx[band]\n",
    "                    \n",
    "                    # moving average for filtering\n",
    "                    def moving_average(a, n=3):\n",
    "                        ret = np.cumsum(a, dtype=float)\n",
    "                        ret[n:] = ret[n:] - ret[:-n]\n",
    "                        return ret[n - 1:] / n\n",
    "                    Zxx_band_pass = moving_average(Zxx_band_pass[0], n=3)\n",
    "        \n",
    "                    # get abs\n",
    "                    Zxx_band_pass = np.abs(Zxx_band_pass)\n",
    "            \n",
    "                    # set to normalized scale\n",
    "                    exp = np.max(Zxx_band_pass)\n",
    "                    Zxx_band_pass = Zxx_band_pass / exp\n",
    "\n",
    "                    # saved to preprocessed_data\n",
    "                    _output.append(Zxx_band_pass)\n",
    "                preprocessed_data[pers_name][loc][aug] = _output\n",
    "                # save to file\n",
    "                np.save(f\"{folder}/{pers_name}_{aug}_{loc}_fft.npy\", _ffts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b186e910-7b90-457f-b20a-0d11768b245d",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6e346df-5e3c-4e62-947d-55b807615a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "from scipy import signal, stats\n",
    "\n",
    "def get_clamped_peak_counts(event, clamp_value=2, debug=None):\n",
    "    ''' get clamped peak count features, never going over the clamp_value\n",
    "\n",
    "        return iterator(int)\n",
    "    '''\n",
    "\n",
    "    # # store a list of clampled values greater than 1\n",
    "    cl_values = list()\n",
    "    \n",
    "    # select different peak threshholds\n",
    "    clamped_values = list()\n",
    "    for i in np.arange(0.2,1,0.2):\n",
    "        peaks_y = list(signal.find_peaks(event, height=i, distance=5)[1][\"peak_heights\"])\n",
    "        \n",
    "        # processing starting of the signal\n",
    "        if event[0] > 0.1:\n",
    "            peaks_y.append(0)\n",
    "            peaks_y.append(event[0])\n",
    "        \n",
    "        # clamp value\n",
    "        clamped_value = min(clamp_value, len(peaks_y))\n",
    "        if (debug is not None) and clamped_value > 1:\n",
    "            cl_values.append(i)\n",
    "            debug[loc][aug].append(i)\n",
    "\n",
    "        clamped_values.append(clamped_value)\n",
    "        \n",
    "    # also return median value\n",
    "    clamped_values.append(0 if len(cl_values) == 0 else sorted(cl_values)[len(cl_values) // 2])\n",
    "    return clamped_values\n",
    "\n",
    "\n",
    "def get_peaks(event):\n",
    "    '''get indices and amplitude of peaks from event\n",
    "\n",
    "        return peaksx(list(int)), peaksy(list(int))\n",
    "    '''\n",
    "    \n",
    "    # get peaks\n",
    "    all_x_peaks = list(signal.find_peaks(event, height=0.1, distance=10)[0])\n",
    "    all_y_peaks = list(event[all_x_peaks])\n",
    "    if event[0] > 0.1:\n",
    "        all_x_peaks.append(0)\n",
    "        all_y_peaks.append(event[0])\n",
    "\n",
    "    # signal end threshhold\n",
    "    all_x_peaks.append(len(event))\n",
    "    all_y_peaks.append(0.1)\n",
    "    return all_x_peaks, all_y_peaks\n",
    "\n",
    "        \n",
    "def get_median_peak_interval_ratio(event, debug=None):\n",
    "    ''' get median eak interval ratio, the first peak divided by the second peak, to estimate energy decay\n",
    "\n",
    "        return int\n",
    "    '''\n",
    "    # get peaks\n",
    "    all_x_peaks, _ = get_peaks(event)\n",
    "    \n",
    "\n",
    "    # get first peak intervals\n",
    "    peak_intervals = list()\n",
    "    for i in range(len(all_x_peaks) - 1):\n",
    "        peak_intervals.append(all_x_peaks[i + 1] - all_x_peaks[i]) \n",
    "\n",
    "    if len(peak_intervals) == 0:\n",
    "        peak_interval = 200\n",
    "    else:\n",
    "        peak_interval = sorted(peak_intervals)[len(peak_intervals) // 2]\n",
    "    debug[loc][aug].append(peak_interval)\n",
    "    return peak_interval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53d07b58-c66c-4811-a086-7a0c21030b55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_peaks = dict()\n",
    "_ratios = dict()\n",
    "_intervals = dict()\n",
    "all_debug_features = (_peaks, _ratios, _intervals)\n",
    "all_debug_features_names = (\"_peaks\", \"_ratios\", \"_intervals\")\n",
    "\n",
    "from scipy import signal, stats, optimize\n",
    "for folder,info in input_files.items():\n",
    "    ppl, augs, locs = info\n",
    "    for pers in ppl:\n",
    "        pers_name, filetype = pers\n",
    "        for debug_features in all_debug_features:\n",
    "            debug_features[pers_name] = {loc: {aug: list() for aug in augs} for loc in locs}\n",
    "        for loc in locs:\n",
    "            for aug in augs:\n",
    "                n_features = list()\n",
    "                for i in range(len(data[pers_name][loc][aug])):\n",
    "                    # get preprocessed event\n",
    "                    event = preprocessed_data[pers_name][loc][aug][i]\n",
    "                    \n",
    "                    # list of features\n",
    "                    features = list()\n",
    "\n",
    "                    \n",
    "                    # get list of clamped peak counts\n",
    "                    for _clamped_peak_counts in get_clamped_peak_counts(event, debug=_peaks[pers_name]):\n",
    "                        features.append(_clamped_peak_counts)\n",
    "                    \n",
    "                    # get median peak interval ratio\n",
    "                    features.append(get_median_peak_interval_ratio(event, debug=_intervals[pers_name]))\n",
    "                    \n",
    "                    # add features to event list\n",
    "                    n_features.append(features)\n",
    "                    \n",
    "                n_features = np.array(n_features)\n",
    "                # print(f\"{pers}_{aug}_{loc}_features.npy\")\n",
    "                np.save(f\"./{folder}/{pers_name}_{aug}_{loc}_features.npy\",n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a02c15-48d1-4ffc-a014-21586e082ee9",
   "metadata": {},
   "source": [
    "### Visualize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c2a4e8c-b513-431a-9878-7c8837f15916",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize_features():\n",
    "    for loc,_d in _intervals.items():\n",
    "        for k,v in _d.items():\n",
    "            plt.hist(v, label=k)\n",
    "        plt.legend()\n",
    "        plt.title(loc)\n",
    "        plt.show()\n",
    "    for folder,info in input_files.items():\n",
    "        ppl, augs, locs = info\n",
    "        for pers in ppl:\n",
    "            pers_name, filetype = pers\n",
    "            for feature_name, debug_features in zip(all_debug_features_names, all_debug_features):\n",
    "                print(feature_name)\n",
    "                fig, axs = plt.subplots(2)\n",
    "                for i, loc in enumerate([\"bottomleft\", \"topleft\"]):\n",
    "                    for aug in debug_features[pers_name][loc].keys():\n",
    "                        features = debug_features[pers_name][loc][aug]\n",
    "                        axs[i].hist(features, label=aug)\n",
    "                    axs[i].legend()\n",
    "                    axs[i].set_title(f'{folder}: {loc}')\n",
    "                fig.set_dpi(100)\n",
    "                plt.show()\n",
    "\n",
    "# visualize_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df79b2a-a5af-47b4-9734-dd637df8e5c4",
   "metadata": {},
   "source": [
    "# Training and Evaluation\n",
    "\n",
    "Using the KAP dataset:\n",
    "- [find accuracy with no LOO (90% dataset for training)](KAP-NO-LOO)\n",
    "- [find accuracy with LOO](KAP-LOO)\n",
    "- [find accuracy with just FFT (no KAFE) with LOO](KAP-FFT)\n",
    "\n",
    "Using the NO_KAA dataset:\n",
    "- [find accuracy with just FFT (no KAFE) with LOO](NO-KAA-FFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5d74b8-a9a4-4a88-adaf-44007938ef45",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd142f04-a6fa-472f-9e8b-0ea64570fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from tuning import svmTuning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "all_ffts = dict()\n",
    "all_features = dict()\n",
    "for folder,info in input_files.items():\n",
    "    ppl, augs, locs = info\n",
    "    augment_keys = {k:n for n,k in enumerate(augs)}\n",
    "    for pers in ppl:\n",
    "        pers_name, filetype = pers\n",
    "        all_features[pers_name] = dict()\n",
    "        all_ffts[pers_name] = dict()\n",
    "        d = all_features[pers_name]\n",
    "        d2 = all_ffts[pers_name]\n",
    "        for loc in locs:\n",
    "            all_features[pers_name][loc] = dict()\n",
    "            all_ffts[pers_name][loc] = dict()\n",
    "            for aug in augs:\n",
    "                _arr = np.load(f\"{folder}/{pers_name}_{aug}_{loc}_features.npy\")\n",
    "                _fft = np.load(f\"{folder}/{pers_name}_{aug}_{loc}_fft.npy\")\n",
    "                \n",
    "\n",
    "                if _arr.shape[0] == 0:\n",
    "                    print(f\"{folder}/{pers_name}_{aug}_{loc}_features.npy\")\n",
    "                scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "                # scale data\n",
    "                _arr = scaler.fit_transform(_arr)\n",
    "                # add labels\n",
    "                _arr = np.c_[_arr, np.ones(_arr.shape[0])*augment_keys[aug]]\n",
    "                _fft = np.c_[_fft, np.ones(_fft.shape[0])*augment_keys[aug]]\n",
    "                # assert that there are no missing features\n",
    "                assert not np.isnan(_arr[:,7:]).any(), f\"contains nan in {folder}/{pers_name}_{aug}_{loc}\"\n",
    "                # save to all_features\n",
    "                d[loc][aug] = _arr\n",
    "                d2[loc][aug] = _fft\n",
    "            \n",
    "\n",
    "# EXAMPLE: M1_ALL_AUGMENTS_ALL_LOCATIONS\n",
    "def tune(X_train, y_train, X_test, y_test):\n",
    "    '''tunes both with linear and rbf. For our purposese, only show rsvm_acc'''\n",
    "    # define classifers\n",
    "    clf_lsvm = SVC(kernel = 'linear')\n",
    "    clf_rsvm = SVC(kernel = 'rbf')\n",
    "    \n",
    "    # tune and print result (tuned_lsvm, tuned_rsvm, lsvm_acc, rsvm_acc)\n",
    "    results = svmTuning(X_train, y_train, X_test, y_test, clf_lsvm, clf_rsvm, 3)\n",
    "    # print(results[2:])\n",
    "    return results\n",
    "\n",
    "def get_dataset(d):\n",
    "    '''return copy of the dataset'''\n",
    "    dataset = []\n",
    "    for loc in d.keys():\n",
    "        for aug in d[loc].keys():\n",
    "            dataset.extend(d[loc][aug])\n",
    "    return np.array(dataset)\n",
    "\n",
    "def get_dataset_location_picky(d, l):\n",
    "    '''returns leave one out based on location of the dataset'''\n",
    "    train = []\n",
    "    test = []\n",
    "    for loc in d.keys():\n",
    "        if l == loc:\n",
    "            # print(\"=\")\n",
    "            for aug in d[loc].keys():\n",
    "                test.extend(d[loc][aug])\n",
    "        else:\n",
    "            for aug in d[loc].keys():\n",
    "                train.extend(d[loc][aug])\n",
    "    return np.array(train), np.array(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8febd-8f6b-4f21-8c63-57c7a7dc82ad",
   "metadata": {},
   "source": [
    "## KAP NO LOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6974e679-a84c-4760-941f-c41a81f48674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['middle', 0.99, 0.02380476142847616]\n",
      "['topright', 0.9666666666666668, 0.039440531887330765]\n",
      "['bottomright', 0.9833333333333334, 0.02886751345948128]\n",
      "['bottomleft', 1.0, 0.0]\n",
      "['topleft', 0.9933333333333334, 0.02]\n"
     ]
    }
   ],
   "source": [
    "# KAP no LOO (90% training)\n",
    "kap_no_loo = list()\n",
    "for pers_name in [\"KAA\"]:\n",
    "\n",
    "    for loc in all_features[pers_name]:\n",
    "        _r = list()\n",
    "        for _i in range(20):\n",
    "            rest_dataset, loc_dataset = get_dataset_location_picky(all_features[pers_name], loc)\n",
    "            locX_train, locX_test, locy_train, locy_test = train_test_split(loc_dataset[:, :-1], loc_dataset[:, -1],\n",
    "                                                      train_size=0.9, random_state=_i, stratify=loc_dataset[:, -1])\n",
    "            restX_train, restX_test, resty_train, resty_test = train_test_split(rest_dataset[:, :-1], rest_dataset[:, -1],\n",
    "                                                      train_size=0.9, random_state=_i, stratify=rest_dataset[:, -1])\n",
    "\n",
    "            _, clf, _, acc = tune(np.concatenate((restX_train,locX_train)), np.concatenate((resty_train,locy_train)), locX_test, locy_test)\n",
    "            _r.append(acc)\n",
    "        accuracy_results_percentage = [loc, np.average(_r), np.std(_r)]\n",
    "        print(accuracy_results_percentage)\n",
    "        kap_no_loo.append(accuracy_results_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dafb941-7202-4a8e-8003-3606094ba428",
   "metadata": {},
   "source": [
    "## KAP LOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "289ecdfc-b17c-4691-ba00-4559d88e01b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['middle', 0.8700000000000001, 0.09303523824635243]\n",
      "['topright', 0.9166666666666667, 0.06280481227138925]\n",
      "['bottomright', 0.8700000000000001, 0.061373175465073225]\n",
      "['bottomleft', 0.4333333333333334, 0.0649786289653931]\n",
      "['topleft', 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# KAP_LOO\n",
    "kap_loo = list()\n",
    "for pers_name in [\"KAA\"]:\n",
    "    for loc in all_features[pers_name]:\n",
    "        _r = list()\n",
    "        for _i in range(20):\n",
    "            rest_dataset, loc_dataset = get_dataset_location_picky(all_features[pers_name], loc)\n",
    "            locX_train, locX_test, locy_train, locy_test = train_test_split(loc_dataset[:, :-1], loc_dataset[:, -1],\n",
    "                                                      train_size=0.9, random_state=_i, stratify=loc_dataset[:, -1])\n",
    "            _, clf, _, acc = tune(rest_dataset[:, :-1], rest_dataset[:, -1], locX_test, locy_test)\n",
    "            _r.append(acc)\n",
    "        accuracy_results_percentage = [loc, np.average(_r), np.std(_r)]\n",
    "        print(accuracy_results_percentage)\n",
    "        kap_loo.append(accuracy_results_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a2bf1e-c7ea-4ba9-8684-e4f493d4f2e6",
   "metadata": {},
   "source": [
    "## KAP FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833bd5d7-ba36-40a8-82c1-229fd79e019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['middle', 0.6199999999999999, 0.09910712498212335]\n"
     ]
    }
   ],
   "source": [
    "# KAP_FFT (no KAFE)\n",
    "kap_fft = list()\n",
    "for pers_name in [\"KAA\"]:\n",
    "    for loc in all_ffts[pers_name]:\n",
    "        _r = list()\n",
    "        for _i in range(20):\n",
    "            rest_dataset, loc_dataset = get_dataset_location_picky(all_ffts[pers_name], loc)\n",
    "            rest_dataset = np.c_[np.abs(rest_dataset[:,:-1]),rest_dataset[:,-1:]]\n",
    "            loc_dataset = np.c_[np.abs(loc_dataset[:,:-1]),loc_dataset[:,-1:]]\n",
    "            _, locX_test, _, locy_test = train_test_split(loc_dataset[:, :-1], loc_dataset[:, -1],\n",
    "                                                      train_size=0.9, random_state=_i, stratify=loc_dataset[:, -1])\n",
    "            _, clf, _, acc = tune(rest_dataset[:, :-1], rest_dataset[:, -1], locX_test, locy_test)\n",
    "            _r.append(acc)\n",
    "        accuracy_results_percentage = [loc, np.average(_r), np.std(_r)]\n",
    "        print(accuracy_results_percentage)\n",
    "        kap_fft.append(accuracy_results_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7fff1a-80ca-4080-909b-6475c25c103d",
   "metadata": {},
   "source": [
    "## NO KAA FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db316d45-7162-4941-adda-48268ace5df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO-KAA-FFT (no KAFE)\n",
    "no_kaa_fft = list()\n",
    "for pers_name in [\"no_KAA\"]:\n",
    "    for loc in all_ffts[pers_name]:\n",
    "        _r = list()\n",
    "        for _i in range(20):\n",
    "            rest_dataset, loc_dataset = get_dataset_location_picky(all_ffts[pers_name], loc)\n",
    "            rest_dataset = np.abs(rest_dataset)\n",
    "            loc_dataset = np.abs(loc_dataset)\n",
    "            _, locX_test, _, locy_test = train_test_split(loc_dataset[:, :-1], loc_dataset[:, -1],\n",
    "                                                      train_size=0.9, random_state=_i, stratify=loc_dataset[:, -1])\n",
    "            _, clf, _, acc = tune(rest_dataset[:, :-1], rest_dataset[:, -1], locX_test, locy_test)\n",
    "            _r.append(acc)\n",
    "        accuracy_results_percentage = [loc, np.average(_r), np.std(_r)]\n",
    "        print(accuracy_results_percentage)\n",
    "        no_kaa_fft.append(accuracy_results_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0283e690-a4da-45b1-b55b-c3e3bfaf006e",
   "metadata": {},
   "source": [
    "## Graphing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2bcb8f-2745-42d7-b1fa-4ccbb216ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "all_results = {\n",
    "    \"KAP no LOO 90% training (ours)\": (kap_no_loo, \"darkgreen\"),\n",
    "    \"KAP with LOO (ours)\": (kap_loo, \"forestgreen\"),\n",
    "    \"KAP no KAFE with LOO\": (kap_fft, \"royalblue\"),\n",
    "    \"no KAA no KAFE with LOO\": (no_kaa_fft, \"#a40000\")\n",
    "}\n",
    "mmap = {\n",
    "    \"middle\": \"M\",\n",
    "    \"topright\": \"T-R\",\n",
    "    \"topleft\": \"T-L\",\n",
    "    \"bottomright\": \"B-R\",\n",
    "    \"bottomleft\": \"B-L\"\n",
    "}\n",
    "width = 0.20  # the width of the bars\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "matplotlib.rc('xtick', labelsize=25) \n",
    "matplotlib.rc('ytick', labelsize=25) \n",
    "plt.figure(figsize=(10,4), dpi=200)\n",
    "for i,results in enumerate(all_results.keys()):\n",
    "    color = all_results[results][1]\n",
    "    locs = [all_results[results][0][multi][0] for multi in range(5)]\n",
    "    locs = [mmap[x] for x in locs]\n",
    "    avg = [all_results[results][0][multi][1]for multi in range(5)]\n",
    "    std = [all_results[results][0][multi][2]for multi in range(5)]\n",
    "    offset = width * i\n",
    "    rects = plt.bar(np.arange(5) + offset, avg, width, label=results, alpha= 0.8, color=color)\n",
    "    plt.errorbar(np.arange(5) + offset, avg, std, linestyle='None', marker='^', c=color)\n",
    "    plt.xticks(np.arange(5) + (width * 1.5), locs)\n",
    "plt.legend(loc='lower left', ncols=1)\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel(\"Accuracy\",fontdict={\"fontsize\": 25})\n",
    "\n",
    "plt.savefig(\"results3.pdf\")\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48236ad-2178-4561-9dce-bcdfd0288a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "average = np.array(np.array(kap_loo)[:,1], dtype=float)\n",
    "kap_loo_average = np.average(average)\n",
    "average = np.array(np.array(kap_fft)[:,1], dtype=float)\n",
    "kap_fft_average = np.average(average)\n",
    "average = np.array(np.array(no_kaa_fft)[:,1], dtype=float)\n",
    "no_kaa_fft_average = np.average(average)\n",
    "\n",
    "print(f'KAP with LOO average is: {kap_loo_average}')\n",
    "print(f'KAP with LOO average - No KAA No KAFE without LOO average is: {kap_loo_average - no_kaa_fft_average}')\n",
    "print(f'KAP with LOO average - No KAFE without LOO average is: {kap_loo_average - kap_fft_average}')\n",
    "print(f'KAP with LOO average std across all locations is: {np.average(np.array(np.array(kap_loo)[:,2], dtype=float))}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574064a-408c-4cee-bdde-9076e0e158b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
